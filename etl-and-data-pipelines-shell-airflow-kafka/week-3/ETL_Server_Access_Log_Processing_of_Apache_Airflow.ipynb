{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ba3f40a",
   "metadata": {},
   "source": [
    "## Assignment: ETL and Data Pipelines with Shell, Airflow and Kafka (Week 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81ccc43",
   "metadata": {},
   "source": [
    "Credit: Coursera\n",
    "\n",
    "This my first assignment of Apache Airflow DAG. The task is shared in [this URL](https://author-ide.skills.network/render?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJtZF9pbnN0cnVjdGlvbnNfdXJsIjoiaHR0cHM6Ly9jZi1jb3Vyc2VzLWRhdGEuczMudXMuY2xvdWQtb2JqZWN0LXN0b3JhZ2UuYXBwZG9tYWluLmNsb3VkL0lCTS1EQjAyNTBFTi1Ta2lsbHNOZXR3b3JrL2xhYnMvQXBhY2hlJTIwQWlyZmxvdy9CdWlsZCUyMGElMjBEQUclMjB1c2luZyUyMEFpcmZsb3cvQnVpbGQlMjBhJTIwREFHJTIwdXNpbmclMjBBaXJmbG93Lm1kIiwidG9vbF90eXBlIjoidGhlaWFkb2NrZXIiLCJhZG1pbiI6ZmFsc2UsImlhdCI6MTY3MjkxOTM5Nn0.eCSPeH0nmI8K2poct5jS0bHiQ2DhxBm_bhtThgqcmVQ). Basically, the task is to create a DAG that has the pipelines of Download, Extract, Transform, Load."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66228334",
   "metadata": {},
   "source": [
    "* Showing the Python version (ignore it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d24037ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.4\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae66cf2a",
   "metadata": {},
   "source": [
    "* The very first step is to import the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e3bf5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "import pendulum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ece53e",
   "metadata": {},
   "source": [
    "* Set the default arguments to create DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f2b215cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_args = {\n",
    "  'owner': 'Iron Man',\n",
    "  'start_date': pendulum.now('Asia/Dhaka'),\n",
    "  'email': ['ironman@somemail.com'],\n",
    "  'email_on_failure': False,\n",
    "  'email_on_retry': False,\n",
    "  'retries': 1,\n",
    "  'retry_delay': timedelta(minutes=5),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9978200f",
   "metadata": {},
   "source": [
    "* Instantiate the DAG class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9977ffbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dag = DAG(\n",
    "  'ETL-server-access-log-processing',\n",
    "  default_args=default_args,\n",
    "  description='ETL server access log processing assignment of coursera course',\n",
    "  schedule=timedelta(days=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92cd316",
   "metadata": {},
   "source": [
    "* download - the first task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06576634",
   "metadata": {},
   "outputs": [],
   "source": [
    " download = BashOperator(\n",
    "  task_id='download',\n",
    "  bash_command='wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0250EN-SkillsNetwork/labs/Apache%20Airflow/Build%20a%20DAG%20using%20Airflow/web-server-access-log.txt',\n",
    "  dag=dag,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de88bdd",
   "metadata": {},
   "source": [
    "* extract - second task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31c5d80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract = BashOperator(\n",
    "  task_id='extract',\n",
    "  bash_command='cut -d\"#\" -f1,4 web-server-access-log.txt > /home/project/airflow/dags/extracted-data.txt',\n",
    "  dag=dag,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc041ef",
   "metadata": {},
   "source": [
    "* transform - third task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b9338be",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = BashOperator(\n",
    "  task_id='transform_data',\n",
    "  bash_command='tr [:lower:] [:upper:] < /home/project/airflow/dags/extracted-data.txt > /home/project/airflow/dags/transformed-data.txt',\n",
    "  dag=dag,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652654fb",
   "metadata": {},
   "source": [
    "* load - final task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0dcbe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "load = BashOperator(\n",
    "  task_id='load',\n",
    "  bash_command='tar -czf /home/project/airflow/dags/ETL_server_access_log_processing.tar.gz /home/project/airflow/dags/extracted-data.txt /home/project/airflow/dags/transformed-data.txt',\n",
    "  dag=dag,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802be75d",
   "metadata": {},
   "source": [
    "* task pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c766327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Task(BashOperator): load>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download >> extract >> transform >> load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2c78b0",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9887b2",
   "metadata": {},
   "source": [
    "### Final Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fdfe562c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Task(BashOperator): load>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "import pendulum\n",
    "\n",
    "default_args = {\n",
    "  'owner': 'Iron Man',\n",
    "  'start_date': pendulum.now('Asia/Dhaka'),\n",
    "  'email': ['ironman@somemail.com'],\n",
    "  'email_on_failure': False,\n",
    "  'email_on_retry': False,\n",
    "  'retries': 1,\n",
    "  'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "  'ETL-server-access-log-processing',\n",
    "  default_args=default_args,\n",
    "  description='ETL server access log processing assignment of coursera course',\n",
    "  schedule=timedelta(days=1)\n",
    ")\n",
    "\n",
    "download = BashOperator(\n",
    "  task_id='download',\n",
    "  bash_command='wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0250EN-SkillsNetwork/labs/Apache%20Airflow/Build%20a%20DAG%20using%20Airflow/web-server-access-log.txt',\n",
    "  dag=dag,\n",
    ")\n",
    "\n",
    "extract = BashOperator(\n",
    "  task_id='extract',\n",
    "  bash_command='cut -d\"#\" -f1,4 web-server-access-log.txt > /home/project/airflow/dags/extracted-data.txt',\n",
    "  dag=dag,\n",
    ")\n",
    "\n",
    "transform = BashOperator(\n",
    "  task_id='transform',\n",
    "  bash_command='tr [:lower:] [:upper:] < /home/project/airflow/dags/extracted-data.txt > /home/project/airflow/dags/transformed-data.txt',\n",
    "  dag=dag,\n",
    ")\n",
    "\n",
    "load = BashOperator(\n",
    "  task_id='load',\n",
    "  bash_command='tar -czf /home/project/airflow/dags/ETL_server_access_log_processing.tar.gz /home/project/airflow/dags/extracted-data.txt /home/project/airflow/dags/transformed-data.txt',\n",
    "  dag=dag,\n",
    ")\n",
    "\n",
    "download >> extract >> transform >> load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d67652",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491a4aed",
   "metadata": {},
   "source": [
    "To add the DAG in the Airflow, follow the below steps.\n",
    "\n",
    "1. Save the above file as ETL_Server_Access_Log_Processing.py\n",
    "2. Run the below command in terminal where Airflow is installed"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3e22d193",
   "metadata": {},
   "source": [
    "cp ETL_Server_Access_Log_Processing.py $AIRFLOW_HOME/dags\n",
    "airflow dags list | grep 'ETL-server-access-log-processing'\n",
    "airflow tasks list ETL-server-access-log-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52364068",
   "metadata": {},
   "source": [
    "You will get the DAG in the webserver of the Airflow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
